{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0ong/-/blob/main/18_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDb4ofxx2E_O"
      },
      "source": [
        "**18장 – 강화 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KEOvyFD2E_U"
      },
      "source": [
        "_이 노트북에는 18장의 모든 샘플 코드와 연습 문제에 대한 해결책이 포함되어 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1UPfvR2E_V"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/18_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TFSU3FCOpKzu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6XYg9weLJZ2f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJtVEqxfpKzw"
      },
      "source": [
        "그리고 TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "이전 챕터에서 했던 것처럼 그림을 더 예쁘게 만들기 위해 기본 글꼴 크기를 정의해 보겠습니다. 또한 Matplotlib 애니메이션을 표시하기 위한 몇 가지 가능한 옵션이 있는데, 여기서는 Javascript 옵션을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4TH3NbpKzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "plt.rc('animation', html='jshtml')\n",
        "\n",
        "import sys\n",
        "# 코랩의 경우 나눔 폰트를 설치합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get -qq -y install fonts-nanum\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])\n",
        "    for fpath in font_files:\n",
        "        fm.fontManager.addfont(fpath)\n",
        "\n",
        "# 나눔 폰트를 사용합니다.\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcoUIRsvpKzy"
      },
      "source": [
        "그리고 `images/rl` 폴더를 만들고(아직 존재하지 않는 경우), 이 노트북을 통해 책에 사용할 그림을 고해상도로 저장하는 데 사용되는 `save_fig()` 함수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQFH5Y9PpKzy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"rl\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "이 챕터는 GPU가 없으면 매우 느려질 수 있으므로 GPU가 있는지 확인하거나 그렇지 않으면 경고를 표시합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU가 감지되지 않았습니다. 신경망은 GPU가 없으면 매우 느릴 수 있습니다.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"런타임 > 런타임 유형 변경으로 이동하여 하드웨어 가속기를 GPU로 선택합니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n59eHPuR2E_d"
      },
      "source": [
        "강화 학습을 위한 다양한 환경을 제공하는 Gymnasium 라이브러리를 설치해 보겠습니다. 또한 곧 사용하게 될 CartPole을 포함한 클래식 제어 환경과 연습에 필요한 Box2D 및 Atari 환경에 필요한 추가 라이브러리도 설치합니다.\n",
        "\n",
        "**중요 노트**\n",
        "\n",
        "* OpenAI는 Gym 라이브러리의 개발과 유지보수를 Farama 재단으로 넘겼습니다(https://farama.org/Announcing-The-Farama-Foundation). 라이브러리 이름은 Gymnasium으로 바뀌었습니다. OpenAI Gym을 쓰는 자리에 대신 사용할 수 있습니다. `gym` 대신 `gymnasium`을 설치하고 `import gymnasium as gym`을 실행하면 됩니다.\n",
        "* 다음 셀을 실행하면 Atari ROM 라이선스에 동의하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6dvmK9c2E_d"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
        "    %pip install -q -U gymnasium\n",
        "    %pip install swig\n",
        "    %pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6USCZjtJ2E_d"
      },
      "source": [
        "# OpenAI ~짐~ Gymnasium 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg6n616v2E_e"
      },
      "source": [
        "이 노트북에서는 강화 학습 알고리즘을 개발하고 비교하기 위한 훌륭한 툴킷인 [gymnasium](https://github.com/Farama-Foundation/Gymnasium)을 사용할 것입니다. 이 도구는 학습 *에이전트*가 상호작용할 수 있는 다양한 환경을 제공합니다. 짐을 임포트하고 새로운 CartPole 환경을 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWMI8vfs2E_e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPGbyFUJ2E_e"
      },
      "source": [
        "CartPole(버전 1)은 왼쪽이나 오른쪽으로 움직일 수 있는 카트와 그 위에 수직으로 놓인 막대로 구성된 매우 간단한 환경입니다. 에이전트는 카트를 왼쪽이나 오른쪽으로 움직여 막대를 똑바로 세워야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYqIbVMo2E_e"
      },
      "source": [
        "**팁**: `gym.envs.registry`는 사용 가능한 모든 환경을 포함하는 디셔너리입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4UgRVAq2E_f"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 처음 몇 개의 환경을 표시합니다.\n",
        "envs = gym.envs.registry\n",
        "sorted(envs.keys())[:5] + [\"...\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmXtLXq52E_f"
      },
      "source": [
        "등록된 값은 환경 사양입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2oEiXxu2E_g"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - CartPole-v1 환경의 사양을 보여줍니다.\n",
        "envs[\"CartPole-v1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT5Ppzn12E_g"
      },
      "source": [
        "`reset()` 메서드를 호출하여 환경을 초기화해 보겠습니다. 이 메서드는 관측값과 추가 정보를 포함할 수 있는 딕셔너리를 반환합니다. 둘 다 환경에 따라 다릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enMIJP_iWAY0"
      },
      "outputs": [],
      "source": [
        "env.reset(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2eQ_zvS2E_g"
      },
      "outputs": [],
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxsEUI-h2E_h"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onokrjZo2E_h"
      },
      "source": [
        "CartPole의 경우 각 관측값은 4개의 실수로 구성된 1D 넘파이 배열로, 카트의 수평 위치, 속도, 극의 각도(0 = 수직) 및 각속도를 나타냅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmJJ-vCj2E_h"
      },
      "source": [
        "환경은 `render()` 메서드를 호출하여 시각화할 수 있습니다. 환경을 생성할 때 `render_mode`를 `\"rgb_array\"`로 설정하면 넘파이 배열이 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gaMWsB12E_h"
      },
      "outputs": [],
      "source": [
        "img = env.render()\n",
        "img.shape  # 높이, 너비, 채널 (3 = 빨강, 초록, 파랑)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZCt0kJC2E_h"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 환경을 렌더링하고 플롯하는 작은 함수를 생성합니다.\n",
        "\n",
        "def plot_environment(env, figsize=(5, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyd_Nn-S2E_i"
      },
      "source": [
        "환경과 상호 작용하는 방법을 살펴보겠습니다. 에이전트는 '행동 공간'(가능한 행동의 집합)에서 행동을 선택해야 합니다. 이 환경의 행동 공간이 어떻게 생겼는지 살펴보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdlFZEBf2E_i"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK-CaP4h2E_i"
      },
      "source": [
        "네, 왼쪽(0)으로 가속하거나 오른쪽(1)으로 가속하는 두 가지 행동만 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygD6ZaNs2E_i"
      },
      "source": [
        "막대가 오른쪽으로 기울어져 있으므로(`obs[2] > 0`) 카트를 오른쪽으로 가속해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cji4kTp12E_i"
      },
      "outputs": [],
      "source": [
        "action = 1  # 오른쪽 가속\n",
        "obs, reward, done, truncated, info = env.step(action)\n",
        "obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCSDZ0ZZ2E_j"
      },
      "source": [
        "이제 카트가 오른쪽으로 이동하고 있음을 알 수 있습니다(`obs[1] > 0`). 기둥은 여전히 오른쪽으로 기울어져 있지만(`obs[2] > 0`), 각속도는 이제 음수(`obs[3] < 0`)이므로 다음 스텝 이후에는 왼쪽으로 기울어질 가능성이 높습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZlqs4GN2E_j"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 환경을 표시합니다.\n",
        "plot_environment(env)\n",
        "save_fig(\"cart_pole_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG9QMbDg2E_j"
      },
      "source": [
        "우리가 시키는 대로 작동하는 것 같습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfe6W-Y2E_j"
      },
      "source": [
        "환경은 에이전트에게 마지막 스텝에서 얼마나 많은 보상을 받았는지도 알려줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8A6Hr8e2E_k"
      },
      "outputs": [],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kJOvIbL2E_k"
      },
      "source": [
        "게임이 끝나면 환경은 `done=True`를 반환합니다. 이 예시에서는 아직 끝나지 않았습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEZIvgPk2E_k"
      },
      "outputs": [],
      "source": [
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMiA06x2E_k"
      },
      "source": [
        "일부 환경 래퍼는 환경을 조기에 중단하고 싶을 수 있습니다. 예를 들어, 시간 제한에 도달하거나 개체가 경계를 벗어난 경우입니다. 이 경우 `truncated`가 `True`로 설정됩니다. 이 예시에서는 아직 중단되지 않았습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrDpksfl2E_k"
      },
      "outputs": [],
      "source": [
        "truncated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4BcVr_I2E_l"
      },
      "source": [
        "마지막으로 `info`는 디버깅이나 훈련에 유용할 수 있는 추가 정보를 제공하는 환경에 특화된 딕셔너리입니다. 예를 들어, 일부 게임에서는 에이전트의 생명력을 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipuff6df2E_l"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3A_VPEg2E_l"
      },
      "source": [
        "환경이 재설정되는 순간부터 완료되거나 잘릴 때까지의 일련의 단계를 \"에피소드\"라고 합니다. 에피소드가 끝날 때(즉, `step()`이 `done=True` 또는 `truncated=True`를 반환하는 경우) 환경을 계속 사용하기 전에 환경을 재설정해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3e5kR5B2E_l"
      },
      "outputs": [],
      "source": [
        "if done or truncated:\n",
        "    obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMgu2ide2E_l"
      },
      "source": [
        "이제 어떻게 하면 투표가 올바르게 유지될 수 있을까요? 이를 위해 _정책_ 을 정의해야 합니다. 이는 에이전트가 각 단계에서 행동을 선택하는 데 사용할 전략입니다. 과거의 모든 행동과 관찰을 사용하여 수행할 행동을 결정할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iej2ikz12E_l"
      },
      "source": [
        "# 간단한 하드코딩 정책"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86eezv112E_m"
      },
      "source": [
        "막대가 왼쪽으로 기울어지면 카트를 왼쪽으로 밀고, 그 반대의 경우도 오른쪽으로 밉니다. 이 방식이 작동하는지 알아보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLYwYevX2E_m"
      },
      "outputs": [],
      "source": [
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs, info = env.reset(seed=episode)\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    totals.append(episode_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1eE53GQ2E_m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(totals), np.std(totals), min(totals), max(totals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgZ91-o2E_m"
      },
      "source": [
        "예상대로 이 전략은 너무 기본적인 전략으로, 막대를 63 스텝 동안만 유지하는 것이 최선입니다. 에이전트가 막대를 200 스텝 동안 유지하면 이 환경이 해결된 것으로 간주됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgIzShO2E_m"
      },
      "source": [
        "하나의 에피소드를 시각화해 봅시다. Matplotlib 애니메이션에 대한 자세한 내용은 [Matplotlib 튜토리얼 노트북](tools_matplotlib.ipynb#Animations)에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEdtJ5PK2E_m"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 한 에피소드의 애니메이션을 표시합니다.\n",
        "\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = matplotlib.animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "def show_one_episode(policy, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    np.random.seed(seed)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        action = policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        if done or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    return plot_animation(frames)\n",
        "\n",
        "show_one_episode(basic_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC_oBl7y2E_n"
      },
      "source": [
        "분명히 시스템이 불안정하고 몇 번만 흔들리면 막대가 너무 기울어져 게임이 끝납니다. 이 보다 더 똑똑해져야 합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atRHcj5g2E_n"
      },
      "source": [
        "# 신경망 정책"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fk9wSM-2E_n"
      },
      "source": [
        "관측값을 입력으로 받고 각 관측값에 대해 취할 행동의 확률을 출력하는 신경망을 만들어 보겠습니다. 행동을 선택하기 위해 네트워크가 각 행동에 대한 확률을 추정해야 합니다. 그런 다음 예상 확률에 따라 무작위로 행동을 선택합니다. CartPole 환경의 경우 가능한 동작이 두 가지(왼쪽 또는 오른쪽)뿐이므로 출력 뉴런은 하나만 필요합니다. 액션 0(왼쪽)의 확률 `p`가 출력되며, 당연히 액션 1(오른쪽)의 확률은 `1 - p`가 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5MRm9Jz2E_n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jaqqbm92E_n"
      },
      "source": [
        "이 특정 환경에서는 각 관측에 환경의 전체 상태가 포함되어 있으므로 과거의 행동과 관측은 안전하게 무시할 수 있습니다. 숨겨진 상태가 있는 경우 환경의 숨겨진 상태를 유추하기 위해 과거의 행동과 관측을 고려해야 할 수 있습니다. 예를 들어, 환경이 카트의 위치만 알려주고 속도는 알려주지 않는다면 현재 속도를 추정하기 위해 현재 관측뿐만 아니라 이전 관측도 고려해야 합니다. 또 다른 예는 관측값에 잡음이 있는 경우로, 과거 몇 개의 관측값을 사용하여 가장 가능성이 높은 현재 상태를 추정할 수 있습니다. 이 문제는 간단합니다. 현재 관측에 노이즈가 없고 환경의 전체 상태를 담고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4ROvpH2E_o"
      },
      "source": [
        "왜 단순히 가장 높은 확률을 가진 행동을 선택하는 것이 아니라 정책 네트워크에서 주어진 확률에 따라 무작위로 행동을 선택하는지 궁금하실 것입니다. 이 접근 방식을 통해 에이전트는 새로운 행동을 _탐험_ 하는 것과 잘 작동하는 것으로 알려진 액션을 _활용_ 하는 것 사이에서 적절한 균형을 찾을 수 있습니다. 비유를 들어보죠. 처음으로 레스토랑에 갔는데 모든 요리가 똑같이 매력적으로 보여서 무작위로 하나를 골랐다고 가정해 보겠습니다. 맛있다고 판명되면 다음에 주문할 확률을 높일 수 있지만, 그 확률을 100%로 높이면 안 됩니다. 그렇지 않으면 먹어본 요리보다 더 맛있을 수 있는 다른 요리를 절대 먹어보지 않게 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9QLM7K2E_o"
      },
      "source": [
        "신경망을 사용하여 왼쪽으로 이동할 확률을 구하는 작은 정책 함수를 작성한 다음, 이를 사용하여 하나의 에피소드를 실행해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JE45q9X2E_o"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 주어진 정책 모델에 대한 애니메이션을 만드는 함수입니다.\n",
        "\n",
        "def pg_policy(obs):\n",
        "    left_proba = model.predict(obs[np.newaxis], verbose=0)[0][0]\n",
        "    return int(np.random.rand() > left_proba)\n",
        "\n",
        "np.random.seed(42)\n",
        "show_one_episode(pg_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMYhS_zj2E_o"
      },
      "source": [
        "네... 꽤 나쁩니다. 신경망은 더 잘하는 법을 배워야 할 것입니다. 먼저 막대가 왼쪽으로 기울어지면 왼쪽으로 이동하고 오른쪽으로 기울어지면 오른쪽으로 이동하는 기본 정책을 학습할 수 있는지 살펴봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-RG-NF2E_o"
      },
      "source": [
        "스스로 더 나은 정책을 학습할 수 있는지 살펴봅시다. 너무 막대가 흔들리지 않는 정책 말입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrNUAuDE2E_o"
      },
      "source": [
        "# 정책 그레이디언트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgR0mSVw2E_p"
      },
      "source": [
        "이 신경망을 훈련하려면 타깃 확률 **y**를 정의해야 합니다. 어떤 행동이 좋으면 그 확률을 높이고, 반대로 나쁘면 확률을 낮춰야 합니다. 하지만 어떤 행동이 좋은지 나쁜지 어떻게 알 수 있을까요? 문제는 대부분의 행동이 지연 효과가 있기 때문에 에피소드에서 점수를 얻거나 잃을 때 어떤 행동이 이 결과에 기여했는지, 즉 마지막 행동이었는지 아니면 마지막 10개 행동이었는지 명확하지 않다는 것입니다. 아니면 마지막 10 스텝? 아니면 50 스텝 전의 행동 하나만 영향을 미쳤나요? 이를 _신용 할당 문제_ 라고 합니다.\n",
        "\n",
        "_정책 그레이디언트_ 알고리즘은 먼저 여러 에피소드를 플레이한 다음, 양의 보상에 가까운 행동은 약간 더 높은 확률로, 음의 보상에 가까운 행동은 약간 더 낮은 확률로 만들어 이 문제를 해결합니다. 먼저 플레이한 다음 다시 돌아가서 자신이 한 행동에 대해 생각합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdJ3b6eO2E_p"
      },
      "source": [
        "모델을 사용하여 한 스텝을 재생하는 함수를 만들어 보겠습니다. 또한 손실과 그레이디언트를 계산할 수 있도록 지금은 어떤 행동을 선택하든 올바른 행동인 것처럼 가정합니다. 지금은 이 그레이디언트를 저장하고 나중에 행동이 얼마나 좋은지 나쁜지에 따라 수정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNy-alAj2E_p"
      },
      "outputs": [],
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, truncated, info = env.step(int(action))\n",
        "    return obs, reward, done, truncated, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr_flQZw2E_p"
      },
      "source": [
        "`left_proba`이 높으면 `action`은 `False`일 가능성이 높습니다(0과 1 사이에서 균일하게 샘플링된 난수는 아마도 `left_proba`보다 크지 않을 것이기 때문입니다). 그리고 `False`는 숫자로 변환할 때 0을 의미하므로 `y_target`은 1 - 0 = 1이 됩니다. 즉, 타깃을 1로 설정하여 왼쪽으로 갈 확률이 100%여야 한다고 가정하고 올바른 행동을 취한 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1_Bil1E2E_p"
      },
      "source": [
        "이제 `play_one_step()` 함수에 의존하여 여러 에피소드를 재생하고 각 에피소드와 각 단계에 대해 모든 보상과 그레이디언트를 반환하는 또 다른 함수를 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcRI5RtY2E_q"
      },
      "outputs": [],
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs, info = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, truncated, grads = play_one_step(\n",
        "                env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "\n",
        "    return all_rewards, all_grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F2sHkXy2E_q"
      },
      "source": [
        "정책 그레이디언트 알고리즘은 모델을 사용하여 에피소드를 여러 번(예: 10회) 재생한 다음 다시 돌아가서 모든 보상을 살펴보고 할인한 후 정규화합니다. 첫 번째 함수는 할인된 보상을 계산하고, 두 번째 함수는 여러 에피소드에 걸쳐 할인된 보상을 정규화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV3xwoq32E_q"
      },
      "outputs": [],
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ZVc_732E_q"
      },
      "source": [
        "3개의 행동이 있고 각 행동 후에 보상을 받습니다. 처음 10, 그다음 0, -50이라고 가정해 보겠습니다. 80%의 할인율을 적용하면 세 번째 행동은 -50(마지막 보상에 대한 전체)을 받지만 두 번째 행동은 -40(마지막 보상에 대한 80%)만 받고, 첫 번째 행동은 -40의 80%(-32)에 첫 번째 보상에 대한 전체 크레딧(+10)을 더하여 -22의 할인된 보상을 받게 됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw7K-V7w2E_q"
      },
      "outputs": [],
      "source": [
        "discount_rewards([10, 0, -50], discount_factor=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qwxUsOM2E_r"
      },
      "source": [
        "모든 에피소드에서 할인된 모든 보상을 정규화하기 위해 모든 할인된 보상의 평균과 표준 편차를 계산하고 각 할인된 보상에서 평균을 빼고 표준 편차로 나눕니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsPg-NUa2E_r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
        "                               discount_factor=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c54Jl_E2E_r"
      },
      "outputs": [],
      "source": [
        "# 코랩의 메모리 부족을 피하기 위해 반복 횟수를 150에서 75로 수정\n",
        "n_iterations = 75\n",
        "# 코랩의 메모리 부족을 피하기 위해 에포크 횟수를 10에서 5로 수정\n",
        "n_episodes_per_update = 5\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzrLibcf2E_r"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재현성을 위해 신경망을 생성하고 환경을 재설정해 보겠습니다.\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "obs, info = env.reset(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oeBeGk_2E_r"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R07f6QAy2E_s"
      },
      "outputs": [],
      "source": [
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "\n",
        "    # 추가 코드 - 훈련 중 일부 디버그 정보를 표시합니다.\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    print(f\"\\r반복: {iteration + 1}/{n_iterations},\"\n",
        "          f\" 평균 보상: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
        "\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIhBGUl82E_s"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 애니메이션을 표시합니다.\n",
        "np.random.seed(42)\n",
        "show_one_episode(pg_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AEXIcKu2E_s"
      },
      "source": [
        "# 추가 자료 - 마르코프 연쇄"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urPwKN5l2E_s"
      },
      "source": [
        "다음 전이 확률은 그림 18-7에 표시된 마르코프 연쇄에 해당합니다. 이 확률적 프로세스를 몇 번 실행하여 어떤 모습인지 확인해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zHvfa22E_t"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "transition_probabilities = [ # shape=[s, s']\n",
        "        [0.7, 0.2, 0.0, 0.1],  # s0에서 s0, s1, s2, s3로\n",
        "        [0.0, 0.0, 0.9, 0.1],  # s1에서 s0, s1, s2, s3로\n",
        "        [0.0, 1.0, 0.0, 0.0],  # s2에서 s0, s1, s2, s3로\n",
        "        [0.0, 0.0, 0.0, 1.0]]  # s3에서 s0, s1, s2, s3로\n",
        "\n",
        "n_max_steps = 1000  # 무한 루프 발생을 방지합니다.\n",
        "terminal_states = [3]\n",
        "\n",
        "def run_chain(start_state):\n",
        "    current_state = start_state\n",
        "    for step in range(n_max_steps):\n",
        "        print(current_state, end=\" \")\n",
        "        if current_state in terminal_states:\n",
        "            break\n",
        "        current_state = np.random.choice(\n",
        "            range(len(transition_probabilities)),\n",
        "            p=transition_probabilities[current_state]\n",
        "        )\n",
        "    else:\n",
        "        print(\"...\", end=\"\")\n",
        "\n",
        "    print()\n",
        "\n",
        "for idx in range(10):\n",
        "    print(f\"실행 #{idx + 1}: \", end=\"\")\n",
        "    run_chain(start_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_lZMqGI2E_t"
      },
      "source": [
        "# 마르코프 결정 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBc7_zGm2E_t"
      },
      "source": [
        "몇 가지 전이 확률, 보상 및 가능한 행동을 정의해 보겠습니다. 예를 들어, 상태 s0에서 행동 a0을 선택하면 확률 0.7로 보상이 +10인 상태 s0으로 이동하고, 확률 0.3으로 보상이 없는 상태 s1로 이동하며, 보상이 없는 상태 s2로 이동하지 않습니다(따라서 전이 확률은 `[0.7, 0.3, 0.0]`이고 보상은 `[+10, 0, 0]`입니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLBP_pGi2E_u"
      },
      "outputs": [],
      "source": [
        "transition_probabilities = [  # shape=[s, a, s']\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
        "    [None, [0.8, 0.1, 0.1], None]\n",
        "]\n",
        "rewards = [  # shape=[s, a, s']\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
        "]\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpJ8TL12E_u"
      },
      "source": [
        "# Q-가치 반복"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9eTH65X2E_u"
      },
      "outputs": [],
      "source": [
        "Q_values = np.full((3, 3), -np.inf)  # 불가능한 동작의 경우 -np.inf\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state, actions] = 0.0  # 가능한 모든 행동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4pg8TPp2E_u"
      },
      "outputs": [],
      "source": [
        "gamma = 0.90  # 할인 계수\n",
        "\n",
        "history1 = []  # 추가 코드 - 아래 그림에 필요\n",
        "for iteration in range(50):\n",
        "    Q_prev = Q_values.copy()\n",
        "    history1.append(Q_prev) # 추가 코드\n",
        "    for s in range(3):\n",
        "        for a in possible_actions[s]:\n",
        "            Q_values[s, a] = np.sum([\n",
        "                    transition_probabilities[s][a][sp]\n",
        "                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
        "                for sp in range(3)])\n",
        "\n",
        "history1 = np.array(history1)  # 추가 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnGCdqxk2E_u"
      },
      "outputs": [],
      "source": [
        "Q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCXTZnXe2E_v"
      },
      "outputs": [],
      "source": [
        "Q_values.argmax(axis=1)  # 각 상태에 대한 최적의 행동"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLTd7aK12E_v"
      },
      "source": [
        "할인 계수 0.90을 사용할 때 이 MDP에 대한 최적의 정책은 상태 s0에 있을 때는 행동 a0을 선택하고, 상태 s1에 있을 때는 행동 a0을 선택한 다음, 마지막으로 상태 s2에 있을 때는 행동 a1(가능한 유일한 행동)을 선택하는 것입니다. 할인 계수를 0.90이 아닌 0.95로 설정하여 다시 시도하면 상태 s1에 대한 최적의 행동이 a2가 된다는 것을 알 수 있습니다. 이는 할인 계수가 더 커서 에이전트가 미래를 더 중요하게 여기기 때문에 더 많은 미래 보상을 받기 위해 즉각적인 페널티를 지불할 준비가 되어 있기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxmi2Ugv2E_v"
      },
      "source": [
        "# Q-러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3UGWrGI2E_v"
      },
      "source": [
        "Q-러닝은 에이전트의 플레이를 지켜보면서(예: 무작위로) Q-가치 추정치를 점진적으로 개선하는 방식으로 작동합니다. Q-가치 추정치가 정확해지면(또는 충분히 근접하면) 최적의 정책은 Q-가치가 가장 높은 행동(즉, 탐욕스러운 정책)을 선택하는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGPgMDM92E_v"
      },
      "source": [
        "환경 내에서 에이전트가 움직이는 것을 시뮬레이션해야 하므로 몇 가지 행동을 수행하고 새로운 상태와 보상을 얻는 함수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dthabYS2E_v"
      },
      "outputs": [],
      "source": [
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOP5UAxZ2E_w"
      },
      "source": [
        "또한 가능한 모든 상태를 여러 번 방문하기만 하면 어떤 정책이라도 될 수 있는 탐험 정책이 필요합니다. 여기서는 상태 공간이 매우 작기 때문에 무작위 정책을 사용하겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8FqQrbB2E_w"
      },
      "outputs": [],
      "source": [
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piB3m5HA2E_w"
      },
      "source": [
        "이제 앞서와 같이 Q-가치를 초기화하고 Q-러닝 알고리즘을 실행해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqh7237X2E_w"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 마찬가지로 Q-가치를 초기화합니다.\n",
        "np.random.seed(42)\n",
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state][actions] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi4650NG2E_w"
      },
      "outputs": [],
      "source": [
        "alpha0 = 0.05  # 초기 학습률\n",
        "decay = 0.005  # 학습률 감쇠\n",
        "gamma = 0.90  # 할인 계수\n",
        "state = 0  # 초기 상태\n",
        "history2 = []  # 추가 코드 - 아래 그림에 필요\n",
        "\n",
        "for iteration in range(10_000):\n",
        "    history2.append(Q_values.copy())  # 추가 코드\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = Q_values[next_state].max()  # 다음 단계에서 탐욕 정책\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= 1 - alpha\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state\n",
        "\n",
        "history2 = np.array(history2)  # 추가 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNv7YNUt2E_w"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 18–9\n",
        "\n",
        "true_Q_value = history1[-1, 0, 0]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
        "axes[0].set_ylabel(\"Q-가치$(s_0, a_0)$\", fontsize=14)\n",
        "axes[0].set_title(\"Q-가치 반복\", fontsize=14)\n",
        "axes[1].set_title(\"Q-러닝\", fontsize=14)\n",
        "for ax, width, history in zip(axes, (50, 10000), (history1, history2)):\n",
        "    ax.plot([0, width], [true_Q_value, true_Q_value], \"k--\")\n",
        "    ax.plot(np.arange(width), history[:, 0, 0], \"b-\", linewidth=2)\n",
        "    ax.set_xlabel(\"반복\", fontsize=14)\n",
        "    ax.axis([0, width, 0, 24])\n",
        "    ax.grid(True)\n",
        "\n",
        "save_fig(\"q_value_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoTGrkf52E_x"
      },
      "source": [
        "# 심층 Q-네트워크"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKzlM8d2E_x"
      },
      "source": [
        "DQN을 구축해 봅시다. 상태가 주어지면 가능한 각 행동에 대해 해당 행동을 수행한 후(결과가 나오기 전) 기대할 수 있는 할인된 미래 보상의 합을 추정합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uyzQIoq2E_x"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "input_shape = [4]  # == env.observation_space.shape\n",
        "n_outputs = 2  # == env.action_space.n\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J1Votd92E_x"
      },
      "source": [
        "이 DQN을 사용하여 작업을 선택하려면 예측된 Q-가치가 가장 큰 행동을 선택하기만 하면 됩니다. 그러나 에이전트가 환경을 탐색하도록 보장하기 위해 확률 `epsilon`으로 랜덤한 행동을 선택합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj9qfhTD2E_x"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)  # 랜덤 행동\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
        "        return Q_values.argmax()  # DQN에 따른 최적의 행동"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TIxwtaV2E_x"
      },
      "source": [
        "재생 버퍼도 필요합니다. 여기에는 에이전트의 경험이 튜플 형태로 포함됩니다: `(obs, action, reward, next_obs, done)`. 이를 위해 `deque` 클래스를 사용할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8wFnakY2E_y"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Dh9EFj2E_y"
      },
      "source": [
        "**참고**: 매우 큰 재생 버퍼의 경우 원형 버퍼를 대신 사용하는 것이 좋을 수 있습니다. 랜덤 액세스 시간이 O(N)이 아닌 O(1)이 되기 떄문입니다. 또는 딥마인드의 [Reverb 라이브러리](https://github.com/deepmind/reverb)를 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9ziPzzr2E_y"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 기본 원형 버퍼 구현\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = np.empty(max_size, dtype=object)\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.randint(self.size, size=batch_size)\n",
        "        return self.buffer[indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TOjhAxc2E_y"
      },
      "source": [
        "그리고 재생 버퍼에서 경험을 샘플링하는 함수를 만들어 보겠습니다. 6개의 넘파이 배열을 반환합니다: `[obs, actions, rewards, next_obs, dones, truncateds]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-qyhvLX2E_y"
      },
      "outputs": [],
      "source": [
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(6)\n",
        "    ]  # [states, actions, rewards, next_states, dones, truncateds]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfMevwuq2E_y"
      },
      "source": [
        "이제 DQN을 사용하여 한 스텝을 재생하고 그 경험을 재생 버퍼에 기록하는 함수를 만들 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9BYB5yf2E_y"
      },
      "outputs": [],
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
        "    return next_state, reward, done, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqHR1t5t2E_z"
      },
      "source": [
        "마지막으로 재생 버퍼에서 일부 경험을 샘플링하고 훈련 스텝을 수행하는 함수를 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oId2Ednk2E_z"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 재현성 및 다음 그림을 생성하기 위한 것입니다.\n",
        "env.reset(seed=42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "rewards = []\n",
        "best_score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs9dcTGy2E_z"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones | truncateds)  # 에피소드가 완료되지 않거나 중단되지 않음\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94zu1RC52E_z"
      },
      "source": [
        "이제 모델을 훈련시켜 보겠습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYq5MA8T2E_z",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 코랩에서 메모리 부족을 피하기 위해 에피소드 횟수를 600에서 300으로 줄입니다.\n",
        "for episode in range(300):\n",
        "    obs, info = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # 추가 코드 - 디버그 정보를 표시하고, 다음 그림을 위한 데이터를 저장하며,\n",
        "    #           지금까지 최고의 모델 가중치를 추적합니다.\n",
        "    print(f\"\\r에피소드: {episode + 1}, 스텝: {step + 1}, eps: {epsilon:.3f}\",\n",
        "          end=\"\")\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "\n",
        "model.set_weights(best_weights)  # 추가 코드 - 최상의 모델 가중치를 복원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3YJQeCv2E_z"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 그림 18-10을 생성하고 저장합니다.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"에피소드\", fontsize=14)\n",
        "plt.ylabel(\"보상의 합\", fontsize=14)\n",
        "plt.grid(True)\n",
        "save_fig(\"dqn_rewards_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPB49v0y2E_0"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 훈련된 DQN이 한 에피소드를 재생하는 애니메이션을 보여줍니다.\n",
        "show_one_episode(epsilon_greedy_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcCOHDRB2E_0"
      },
      "source": [
        "전혀 나쁘지 않아요! 😀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ERA4j6G2E_0"
      },
      "source": [
        "## 고정 Q-가치 타깃"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vxCKbDi2E_0"
      },
      "source": [
        "온라인 DQN을 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-jPNE0A2E_0"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 동일한 DQN 모델을 생성합니다.\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exXMK2id2E_0"
      },
      "source": [
        "이제 대상 DQN을 생성합니다. 이는 그냥 온라인 DQN의 복제본입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSowDG3X2E_0"
      },
      "outputs": [],
      "source": [
        "target = tf.keras.models.clone_model(model)  # 모델의 아키텍처 복제\n",
        "target.set_weights(model.get_weights())  # 가중치 복사"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3di4ysro2E_1"
      },
      "source": [
        "다음으로 `# <= 변경됨`으로 표시된 줄을 제외하고 위와 동일한 코드를 사용합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BesmpNMm2E_1"
      },
      "outputs": [],
      "source": [
        "env.reset(seed=42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)  # 재생 버퍼를 초기화합니다.\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "    next_Q_values = target.predict(next_states, verbose=0)  # <= 변경됨\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones | truncateds)  # 에피소드가 완료되지 않거나 중단되지 않음\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpftRsKu2E_1"
      },
      "source": [
        "여기에서도 `# <= 변경됨`으로 표시된 줄을 제외하고는 이전과 동일한 코드입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUR5VEM72E_1"
      },
      "outputs": [],
      "source": [
        "# 코랩에서 메모리 부족을 피하기 위해 에피소드 횟수를 600에서 300으로 줄입니다.\n",
        "for episode in range(300):\n",
        "    obs, info = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info, truncated = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # 추가 코드 - 디버그 정보를 표시하고, 다음 그림을 위한 데이터를 저장하며,\n",
        "    #           지금까지 최고의 모델 가중치를 추적합니다.\n",
        "    print(f\"\\r에피소드: {episode + 1}, 스텝: {step + 1}, eps: {epsilon:.3f}\",\n",
        "          end=\"\")\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "        if episode % 50 == 0:                        # <= 변경됨\n",
        "            target.set_weights(model.get_weights())  # <= 변경됨\n",
        "\n",
        "    # 또는 각 스텝에서 소프트 업데이트를 수행할 수도 있습니다:\n",
        "    #if episode > 50:\n",
        "        #training_step(batch_size)\n",
        "        #target_weights = target.get_weights()\n",
        "        #online_weights = model.get_weights()\n",
        "        #for index, online_weight in enumerate(online_weights):\n",
        "        #    target_weights[index] = (0.99 * target_weights[index]\n",
        "        #                             + 0.01 * online_weight)\n",
        "        #target.set_weights(target_weights)\n",
        "\n",
        "model.set_weights(best_weights)  # 추가 코드 - 최상의 모델 가중치를 복원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tGV_tHs2E_1"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 학습 곡선을 표시합니다.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"에피소드\", fontsize=14)\n",
        "plt.ylabel(\"보상의 합\", fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14p77AEs2E_2",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 훈련된 DQN이 한 에피소드를 재생하는 애니메이션을 보여줍니다.\n",
        "show_one_episode(epsilon_greedy_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Ipg18A2E_2"
      },
      "source": [
        "## 더블 DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCWfVfR2E_2"
      },
      "source": [
        "코드는 `training_step()` 함수에서 `변경된 섹션`을 제외하고는 고정 Q-가치 타깃의 경우와 완전히 동일합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh0xF1Qu2E_3"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "target = tf.keras.models.clone_model(model)  # 모델의 아키텍처 복제\n",
        "target.set_weights(model.get_weights())  # 가중치 복사\n",
        "\n",
        "env.reset(seed=42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "\n",
        "    #################### 변경된 섹션 ####################\n",
        "    next_Q_values = model.predict(next_states, verbose=0)  # ≠ target.predict()\n",
        "    best_next_actions = next_Q_values.argmax(axis=1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n",
        "                        ).sum(axis=1)\n",
        "    #########################################################\n",
        "\n",
        "    runs = 1.0 - (dones | truncateds)  # 에피소드가 완료되지 않거나 중단되지 않음\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "# 코랩에서 메모리 부족을 피하기 위해 에피소드 횟수를 600에서 300으로 줄입니다.\n",
        "for episode in range(300):\n",
        "    obs, info = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info, truncated = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    print(f\"\\r에피소드: {episode + 1}, 스텝: {step + 1}, eps: {epsilon:.3f}\",\n",
        "          end=\"\")\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "        if episode % 50 == 0:\n",
        "            target.set_weights(model.get_weights())\n",
        "\n",
        "model.set_weights(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXB_Ky-J2E_3"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 학습 곡선을 표시합니다.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"에피소드\", fontsize=14)\n",
        "plt.ylabel(\"보상의 합\", fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc8Iy0MP2E_3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 훈련된 DQN이 한 에피소드를 재생하는 애니메이션을 보여줍니다.\n",
        "show_one_episode(epsilon_greedy_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vchVylgP2E_4"
      },
      "source": [
        "# 듀얼링 더블 DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw4UGEfT2E_4"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
        "\n",
        "input_states = tf.keras.layers.Input(shape=[4])\n",
        "hidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = tf.keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\n",
        "                                            keepdims=True)\n",
        "Q_values = state_values + advantages\n",
        "model = tf.keras.Model(inputs=[input_states], outputs=[Q_values])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9OOxlBF2E_4"
      },
      "source": [
        "나머지는 이전과 동일한 코드입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSCXz4op2E_4"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델 훈련\n",
        "\n",
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-3)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "target = tf.keras.models.clone_model(model)  # 모델의 아키텍처 복제\n",
        "target.set_weights(model.get_weights())  # 가중치 복사\n",
        "\n",
        "env.reset(seed=42)\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "rewards = []\n",
        "best_score = 0\n",
        "\n",
        "# 코랩에서 메모리 부족을 피하기 위해 에피소드 횟수를 600에서 300으로 줄입니다.\n",
        "for episode in range(300):\n",
        "    obs, info = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info, truncated = play_one_step(env, obs, epsilon)\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    print(f\"\\r에피소드: {episode + 1}, 스텝: {step + 1}, eps: {epsilon:.3f}\",\n",
        "          end=\"\")\n",
        "    rewards.append(step)\n",
        "    if step >= best_score:\n",
        "        best_weights = model.get_weights()\n",
        "        best_score = step\n",
        "\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "        if episode % 50 == 0:\n",
        "            target.set_weights(model.get_weights())\n",
        "\n",
        "model.set_weights(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5xP6yre2E_5"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이 셀은 학습 곡선을 표시합니다.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"에피소드\", fontsize=14)\n",
        "plt.ylabel(\"보상의 합\", fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjpq3CV02E_5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 훈련된 DQN이 한 에피소드를 재생하는 애니메이션을 보여줍니다.\n",
        "show_one_episode(epsilon_greedy_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX55mWEK2E_5"
      },
      "source": [
        "꽤 안정된 에이전트처럼 보입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP7Q2Wom2E_5"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEk-ccr2E_6"
      },
      "source": [
        "# 연습문제 해답"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBF1n4YQ2E_6"
      },
      "source": [
        "## 1. to 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Ni6ZER2E_6"
      },
      "source": [
        "부록 A 참조."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWOYRo62E_6"
      },
      "source": [
        "## 8.\n",
        "_문제: 정책 그레이디언트를 사용해 ~OpenAI 짐~ Gymnasium의 LunarLander-v2 환경을 해결해보세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVeKjo_C2E_8"
      },
      "source": [
        "먼저 LunarLander-v2 환경을 만들어 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkRkLij2E_8"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMg9GmM2E_8"
      },
      "source": [
        "입력은 8차원입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_bK-XnG2E_8"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRTgdbDv2E_8"
      },
      "outputs": [],
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aegSan5r2E_9"
      },
      "source": [
        "[소스 코드](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py)를 보면 8D 관측(x, y, h, v, a, w, l, r)이 각각 다음에 해당합니다:\n",
        "* x,y: 우주선의 좌표. (0, 1.4) 근처의 랜덤한 위치에서 시작하고 (0, 0)에 있는 목적지 근처에 내려야 합니다.\n",
        "* h,v: 우주선의 수평, 수직 속도. 랜덤한 적은 속도로 시작합니다.\n",
        "* a,w: 우주선의 각도와 각속도.\n",
        "* l,r: 왼쪽이나 오른쪽 다리가 땅에 닿았는지(1.0) 아닌지(0.0) 여부."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L3YuqCg2E_9"
      },
      "source": [
        "행동 공간은 이산적이며 4개의 가능한 행동이 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WlBfxQx2E_9"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jnp1Phd2E_9"
      },
      "source": [
        "[LunarLander-v2 설명](https://gymnasium.farama.org/environments/box2d/lunar_lander/)을 보면 이 행동은 다음과 같습니다:\n",
        "* 아무것도 하지 않음\n",
        "* 왼쪽 방향 엔진을 켬\n",
        "* 주 엔진을 켬\n",
        "* 오른쪽 방향 엔진을 켬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjyAvAPt2E_9"
      },
      "source": [
        "(행동마다 하나씩) 4개의 출력 뉴런을 가진 간단한 정책 네트워크를 만들어 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lkj6hDO2E_9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_inputs = env.observation_space.shape[0]\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n_outputs, activation=\"softmax\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUMrNkme2E_9"
      },
      "source": [
        "출력 층에 CartPole-v1 환경처럼 시그모이드 활성화 함수를 사용하지 않고 대신에 소프트맥스 활성화 함수를 사용합니다. CartPole-v1 환경은 두 개의 행동만 있어서 이진 분류 모델이 맞기 때문입니다. 하지만 두 개 이상의 행동이 있으므로 다중 분류 모델이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IemdFKDq2E_-"
      },
      "source": [
        "그다음 CartPole-v1 정책 그레이디언트 코드에서 정의한 `play_one_step()`와 `play_multiple_episodes()` 함수를 재사용합니다. 하지만 다중 분류 모델에 맞게 `play_one_step()`를 조금 수정하겠습니다. 그다음 수정된 `play_one_step()` 를 호출하고, 우주선이 최대 스텝 횟수 전에 랜딩하지 못하면 (또는 부서지면) 큰 페널티를 부여하도록 `play_multiple_episodes()` 함수를 수정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXbsnP962E_-"
      },
      "outputs": [],
      "source": [
        "def lander_play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        probas = model(obs[np.newaxis])\n",
        "        logits = tf.math.log(probas + tf.keras.backend.epsilon())\n",
        "        action = tf.random.categorical(logits, num_samples=1)\n",
        "        loss = tf.reduce_mean(loss_fn(action, probas))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info, truncated = env.step(action[0, 0].numpy())\n",
        "    return obs, reward, done, truncated, grads\n",
        "\n",
        "def lander_play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs, info = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, truncated, grads = lander_play_one_step(\n",
        "                env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done or truncated:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cwXK1nj2FAC"
      },
      "source": [
        "앞에서와 동일한 `discount_rewards()`와 `discount_and_normalize_rewards()` 함수를 사용합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWXhi5Yi2FAC"
      },
      "outputs": [],
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPDdTc0f2FAC"
      },
      "source": [
        "이제 몇 개의 하이퍼파라미터를 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrVOSJMP2FAD"
      },
      "outputs": [],
      "source": [
        "# 코랩의 메모리 부족을 피하기 위해 반복 횟수를 200에서 50으로 수정\n",
        "n_iterations = 50\n",
        "n_episodes_per_update = 16\n",
        "# 코랩의 메모리 부족을 피하기 위해 최대 스텝 횟수를 1000에서 200으로 수정\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cZHu0tg2FAD"
      },
      "source": [
        "여기서도 다중 분류 모델이기 때문에 이진 크로스 엔트로피가 아니라 범주형 크로스 엔트로피를 사용해야 합니다. 또한 `lander_play_one_step()` 함수가 클래스 확률이 아니라 클래스 레이블로 타깃을 설정하기 때문에 `sparse_categorical_crossentropy()` 손실 함수를 사용해야 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmUi97p62FAD"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.005)\n",
        "loss_fn = tf.keras.losses.sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmze_KAO2FAD"
      },
      "source": [
        "모델을 훈련할 준비가 되었네요. 시작해 보죠!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG57ryhR2FAD"
      },
      "outputs": [],
      "source": [
        "env.reset(seed=42)\n",
        "\n",
        "mean_rewards = []\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = lander_play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n",
        "    print(f\"\\r반복: {iteration + 1}/{n_iterations},\"\n",
        "          f\" 평균 보상: {mean_reward:.1f}  \", end=\"\")\n",
        "    mean_rewards.append(mean_reward)\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpADyZwR2FAD"
      },
      "source": [
        "학습 곡선을 그려 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IpH9DUe2FAD"
      },
      "outputs": [],
      "source": [
        "plt.plot(mean_rewards)\n",
        "plt.xlabel(\"에피소드\")\n",
        "plt.ylabel(\"평균 보상\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQMWVBG2FAE"
      },
      "source": [
        "결과를 확인해 보죠!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCvD4quV2FAE"
      },
      "outputs": [],
      "source": [
        "def lander_render_policy_net(model, n_max_steps=500, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        probas = model(obs[np.newaxis])\n",
        "        logits = tf.math.log(probas + tf.keras.backend.epsilon())\n",
        "        action = tf.random.categorical(logits, num_samples=1)\n",
        "        obs, reward, done, truncated, info = env.step(action[0, 0].numpy())\n",
        "        if done or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8BUWxr-2FAE"
      },
      "outputs": [],
      "source": [
        "frames = lander_render_policy_net(model, seed=42)\n",
        "plot_animation(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DLvNbqc2FAE"
      },
      "source": [
        "꽤 괜찮군요. 더 오래 훈련하거나 하이퍼파라미터를 튜닝하여 200을 넘을 수 있는지 확인해 보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRLnvKVw2FAE"
      },
      "source": [
        "## 9.\n",
        "_문제: 더블 듀얼링 DQN을 사용하여 유명한 아타리 브레이크아웃(Breakout) 게임(`\"ALE/Breakout-v5\"`)에서 사람의 수준을 뛰어 넘을 수 있는 에이전트를 훈련해 보세요. 관측은 이미지입니다. 작업을 단순화하기 위해 흑백으로 변환(즉, 채널 축을 따라 평균)합니다. 그다음 재생할 수 있을 만큼 크지만 그 이상되지 않도록 자르고 다운샘플링합니다. 개별 이미지만으로는 공과 패들(paddle)이 어느 방향으로 가고 있는지 알 수 없으므로 두세 개의 연속된 이미지를 병합하여 각 상태를 만들어야 합니다. 마지막으로 DQN은 합성곱 층으로 대부분 구성되어야 합니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ws7Xwl32FAE"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEMkyxbg2FAE"
      },
      "source": [
        "[paperswithcode.com에서 아타리 게임의 최신 기술](https://paperswithcode.com/task/atari-games)을 확인하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303trSif2FAE"
      },
      "source": [
        "## 10.\n",
        "_문제: 10만 원 정도 여유가 있다면 라즈베리 파이 3와 저렴한 로보틱스 구성품을 구입해 텐서플로를 설치하고 실행할 수 있습니다! 예를 들어 루카스 비월드의 재미있는 [포스트](https://homl.info/2)를 참고하거나, GoPiGo42나 BrickPi43를 둘러보세요. 간단한 작업부터 시작해보세요. 예를 들어 (조도 센서가 있다면) 로봇이 밝은 쪽으로 회전하거나 (초음파 센서가 있다면) 가까운 물체가 있는 쪽으로 움직이도록 해보세요. 그다음 딥러닝을 사용해보세요. 예를 들어 로봇에 카메라가 있다면 객체 탐지 알고리즘을 구현해 사람을 감지하고 가까이 다가가게 만들 수 있습니다. 강화 학습을 사용해 목표를 달성하기 위해 모터 사용법을 스스로 학습할 수도 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8-sInsl2FAE"
      },
      "source": [
        "이제 여러분 차례입니다. 도전적이고 창의적으로, 무엇보다도 인내심을 가지고 한 발씩 나아가세요. 여러분은 할 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwjOqFwDIfGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "95c485e91159f3a8b550e08492cb4ed2557284663e79130c96242e7ff9e65ae1"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}